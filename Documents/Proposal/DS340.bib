Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{VanDenOord,
abstract = {Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative filtering. However, this approach suf- fers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantita- tively and qualitatively on the Million Song Dataset. We showthat using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep con- volutional neural networks significantly outperforming the traditional approach.},
author = {van den Oord, Aaron and Dieleman, Sander and Schrauwen, Benjamin},
doi = {10.1109/MMUL.2011.34.van},
file = {:C$\backslash$:/Users/Kyle/Downloads/5004-deep-content-based-music-recommendation.pdf:pdf},
isbn = {9780769535029},
issn = {10495258},
journal = {Electronics and Information Systems department (ELIS)},
pages = {9},
title = {{Deep content-based music recommendation}},
url = {http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf},
year = {2013}
}
@article{MarkusFruthwirth2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Munz, Ernst Dietrich},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Kyle/Downloads/fru{\_}wirn01-1.pdf:pdf},
isbn = {9780874216561},
issn = {07221541},
journal = {Nervenheilkunde},
keywords = {Education reform,Personnel requirements,Psychiatry,Psychotherapist},
number = {10},
pages = {800--805},
pmid = {15991970},
title = {{Psychotherapie in der Psychiatrie}},
volume = {36},
year = {2017}
}
@article{Nam2015,
abstract = {Feature learning and deep learning have drawn great attention in recent years as a way of transforming input data into more effective representations using learning algorithms. Such interest has grown in the area of music information retrieval (MIR) as well, particularly in music audio classification tasks such as auto-tagging. In this paper, we present a two-stage learning model to effectively predict multiple labels from music audio. The first stage learns to project local spectral patterns of an audio track onto a high-dimensional sparse space in an unsupervised manner and summarizes the audio track as a bag-of-features. The second stage successively performs the unsupervised learning on the bag-of-features in a layer-by-layer manner to initialize a deep neural network and finally fine-tunes it with the tag labels. Through the experiment, we rigorously examine training choices and tuning parameters, and show that the model achieves high performance on Magnatagatune, a popularly used dataset in music auto-tagging.},
archivePrefix = {arXiv},
arxivId = {1508.04999},
author = {Nam, Juhan and Herrera, Jorge and Lee, Kyogu},
eprint = {1508.04999},
file = {:C$\backslash$:/Users/Kyle/Downloads/BagOfFeaturesBasedRecommendation.pdf:pdf},
pages = {1--9},
title = {{A Deep Bag-of-Features Model for Music Auto-Tagging}},
url = {http://arxiv.org/abs/1508.04999},
year = {2015}
}
@article{Simpson2015,
abstract = {Identification and extraction of singing voice from within musical mixtures is a key challenge in source separation and machine audition. Recently, deep neural networks (DNN) have been used to estimate 'ideal' binary masks for carefully controlled cocktail party speech separation problems. However, it is not yet known whether these methods are capable of generalizing to the discrimination of voice and non-voice in the context of musical mixtures. Here, we trained a convolutional DNN (of around a billion parameters) to provide probabilistic estimates of the ideal binary mask for separation of vocal sounds from real-world musical mixtures. We contrast our DNN results with more traditional linear methods. Our approach may be useful for automatic removal of vocal sounds from musical mixtures for 'karaoke' type applications. Index},
archivePrefix = {arXiv},
arxivId = {1504.04658},
author = {Simpson, Andrew J R and Roma, Gerard and Plumbley, Mark D.},
doi = {10.1007/978-3-319-22482-4\_50},
eprint = {1504.04658},
file = {:C$\backslash$:/Users/Kyle/Downloads/VocalExtraction.pdf:pdf},
isbn = {9783319224817},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolution,Deep learning,Source separation,Supervised learning},
pages = {429--436},
pmid = {15991970},
title = {{Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network}},
volume = {9237},
year = {2015}
}
@article{InternationalSocietyforMusicInformationRetrievalConference.Conference14:2013:Curitiba2013,
abstract = {The spherical k-means algorithm, i.e., the k-means algorithm with cosine similarity, is a popular method for clustering high-dimensional text data. In this algorithm, each document as well as each cluster mean is represented as a high-dimensional unit-length vector. However, it has been mainly used in hatch mode. Thus is, each cluster mean vector is updated, only after all document vectors being assigned, as the (normalized) average of all the document vectors assigned to that cluster. This paper investigates an online version of the spherical k-means algorithm based on the well-known winner-take-all competitive learning. In this online algorithm, each cluster centroid is incrementally updated given a document. We demonstrate that the online spherical k-means algorithm can achieve significantly better clustering results than the batch version, especially when an annealing-type learning rate schedule is used. We also present heuristics to improve the speed, yet almost without loss of clustering quality.},
author = {Zhong, Shi},
doi = {10.1109/IJCNN.2005.1556436},
file = {:C$\backslash$:/Users/Kyle/Downloads/1.pdf:pdf},
isbn = {0780390482},
issn = {14673045},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {K,Technology and Engineering,feature learning,means,multiple timescales,music information retrieval},
pages = {3180--3185},
pmid = {21555788},
title = {{Efficient online spherical k-means clustering}},
volume = {5},
year = {2005}
}
@article{Pons2017,
abstract = {The focus of this work is to study how to efficiently tailor Convolutional Neural Networks (CNNs) towards learning timbre representations from log-mel magnitude spectrograms. We first review the trends when designing CNN architectures. Through this literature overview we discuss which are the crucial points to consider for efficiently learning timbre representations using CNNs. From this discussion we propose a design strategy meant to capture the relevant time-frequency contexts for learning timbre, which permits using domain knowledge for designing architectures. In addition, one of our main goals is to design efficient CNN architectures -- what reduces the risk of these models to over-fit, since CNNs' number of parameters is minimized. Several architectures based on the design principles we propose are successfully assessed for different research tasks related to timbre: singing voice phoneme classification, musical instrument recognition and music auto-tagging.},
archivePrefix = {arXiv},
arxivId = {1703.06697},
author = {Pons, Jordi and Slizovskaia, Olga and Gong, Rong and G{\'{o}}mez, Emilia and Serra, Xavier},
eprint = {1703.06697},
file = {:C$\backslash$:/Users/Kyle/Downloads/TimbreAnalysisCNN.pdf:pdf},
isbn = {9780992862671},
title = {{Timbre Analysis of Music Audio Signals with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1703.06697},
year = {2017}
}
